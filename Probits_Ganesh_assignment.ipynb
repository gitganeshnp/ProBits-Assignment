{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docxpy\n",
    "import re\n",
    "from docx import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining Email using Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ganesh.nethip@gmail.com']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = docxpy.process('C:\\\\Users\\\\Administrator\\\\Probits\\\\Ganesh_Resume.docx')\n",
    "text\n",
    "emails = re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", text)\n",
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'ganesh.nethip@gmail.com'\n"
     ]
    }
   ],
   "source": [
    "doc = docxpy.DOCReader('C:\\\\Users\\\\Administrator\\\\Probits\\\\Ganesh_Resume.docx')\n",
    "doc.process()  # processing the file\n",
    "hyperlinks = doc.data['links']\n",
    "print(hyperlinks[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining only skills & Technologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  Languages- R',\n",
       " ' Python',\n",
       " ' SQL',\n",
       " ' HTML & CSS',\n",
       " ' Java Tools and IDEs- Tableau',\n",
       " ' Power BI',\n",
       " ' Jupyter',\n",
       " ' Eclipse',\n",
       " ' Weka',\n",
       " ' Spyder Technology Skills - Statistics',\n",
       " ' Machine Learning',\n",
       " ' Tensorflow',\n",
       " ' Big Data',\n",
       " ' Data Scraping',\n",
       " ' Pyspark ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = Document('C:\\\\Users\\\\Administrator\\\\Probits\\\\Ganesh_Resume.docx')\n",
    "paragraphs=document.paragraphs\n",
    "dict1={}\n",
    "\n",
    "para1=''\n",
    "str1=\"\"\n",
    "\n",
    "for para in document.paragraphs:\n",
    "\n",
    "    #print(paragraphs[p].text)\n",
    "    text=para.text.replace('\\t','')\n",
    "    if para.style.name == \"Heading 1\":\n",
    "        str1=\"\"\n",
    "        para1=text\n",
    "        #print(para1)\n",
    "    else:\n",
    "       # print(str1)\n",
    "        str1=str1+' '+text\n",
    "        \n",
    "    if para1=='':\n",
    "        para1='basic_info'\n",
    "\n",
    "    #print(para1)\n",
    "    dict1[para1]=str1\n",
    "    #print(dict1)\n",
    "    #print(\"--------------\")   \n",
    "dict1['Skills & Technologies'].split(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The above code will be modified based on the input document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'basic_info': ' N P GANESH  MAIL:                           Bengaluru                                PHONE: +91 9704242880                                                               Education   Professional Experience  Employer - Udeels Technologies, Bengaluru  Role - Associate Software Engineer (May 2017 to May 2018)                                                                                                                  Project Title:  AGGREGATOR BIGDATA PLATFORM DEVELOPMENT      My responsibility was to develop, debug text and deployment of Front-End modules, Python based functionalities. After retrieving data from database using python modules, I had parsed the user information and had shown the data in the clients like web, android applications Using Java Script and HTML/CSS technologies had developed the front-end which interacts with business data and customer data  Skills  Languages             - R, Python, SQL, HTML & CSS, Java     Tools and IDEs     - Tableau, Power BI, Jupyter, Eclipse, Weka, Spyder     Technology Skills - Statistics, Machine Learning, Tensorflow, Big Data, Data Scraping, Pyspark  Certifications:  Udemy           -     Machine Learning A-Z (Pursuing)     HP                  -    Certified for learning and completion of Core Java     NVIDIA         -    Parallel programming with NVIDIA GPU     IBNC INDIA -    Certified for the undergone training on Cisco Networking (CCNA)     Datacamp      -    R Programming  Publications & Profiles:  GitHub -  Article  -  LinkedIn -  Paper    - Presented a paper in National Conference of Information and Communication Technology in 2017   Under Graduation Project:  Project Title: Prefetching on Storage servers through mining access patterns on blocks            Provides File security for the end users.         Introduced Client File System        To overcome the issues on data security through X-step algorithm and by developing client file system  Internship Project:  Project Title:  Implementation of WAN Design                 Using this concept it is possible for the networker to check the incoming and the outgoing traffic and to maintain    network security.  In this logic we use the multiple Routing Protocols in different areas of the countrywide WAN.  To gain knowledge related to the project  Learning about new Routing and Switching  Observing Network traffic     Course Projects:  Hack the News, predicted news as propaganda or non-propaganda (Text Analytics). Built 9 ML Algorithms to select a best model fit. Chatbot using TFIDF and Cosine similarity  Analysis and Insights on IPL and ODI Dataset using R and Python  Dashboard creation for IPL Dataset and USA Homicide Dataset using Tableau Simple Analysis on Bank Database and HR DB using Oracle SQL Scraped live tweets and performed sentiment analysis. Analysis and Visualization of Terror Data for last 45 years Image Classification using Keras and Tensorflow Market Basket Analysis for a retail store             '}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
